{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42e78c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b46fcdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "##导入数据\n",
    "height_source = pd.read_csv('E:\\实验数据\\python\\RNN_data\\data_24.csv')\n",
    "input_source = pd.read_csv('E:\\实验数据\\python\\RNN_data\\input_24.csv')\n",
    "height_array = height_source.values\n",
    "input_array = input_source.values\n",
    "\n",
    "##转换成array\n",
    "height_array = height_array.astype('double')\n",
    "input_array = input_array.astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d6985a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##划分训练集和测试集\n",
    "n_train = 50\n",
    "train_input = input_array[:n_train, :]\n",
    "train_height = height_array[:n_train]\n",
    "# test_input = input_array[n_train:50, :]\n",
    "# test_height = height_array[n_train:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9aedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "##归一化\n",
    "x_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "y_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "train_x = x_scaler.fit_transform(train_input)\n",
    "train_y = y_scaler.fit_transform(train_height)\n",
    "\n",
    "# test_x = x_scaler.fit_transform(test_input)\n",
    "# test_y = y_scaler.fit_transform(test_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead73f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#转换成tensor\n",
    "train_X = torch.tensor(train_x, dtype=torch.double)\n",
    "train_Y = torch.tensor(train_y)\n",
    "# test_X = torch.tensor(test_x)\n",
    "# test_Y = torch.tensor(test_y)\n",
    "\n",
    "# 分成batch\n",
    "train_X = train_X.reshape(-1, 5, 2) #时间步长为5 每次的输入是u1 u2\n",
    "train_Y = train_Y.reshape(-1, 5, 1) #时间步长为5 每次的labels 是高度\n",
    "# test_X = test_X.reshape(-1, 5, 2) \n",
    "# test_Y = test_Y.reshape(-1, 5, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc08020",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_STEP = 5  #时间步长为5 \n",
    "INPUT_SIZE = 2 #输入特征数，相当于X的维度\n",
    "LR = 0.005  #learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28316fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=INPUT_SIZE,\n",
    "            #hidden_size，num_layers 都是对网络的设置，与输入数据无关，设置相对自由\n",
    "            hidden_size=5,     # rnn hidden unit\n",
    "            num_layers=1,       # number of rnn layer\n",
    "            batch_first=True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(5, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        # x (batch, time_step, input_size)\n",
    "        # h_state (n_layers, batch, hidden_size) torch.Size([1, 50, 32])\n",
    "        # r_out (batch, time_step, hidden_size) torch.Size([50, 10, 32])\n",
    "        # x包含10个时间步的数据，但h_state是最后一个时间步的h_state，r_out包含每一个时间步的output（因为每个时间步都会有输出）\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "    \n",
    "        outs = []    # save all predictions\n",
    "        #时间步(TIME_STEP)决定了h_state向后传递几步，也就是outs能使用到多少步的时序数据\n",
    "        for time_step in range(TIME_STEP): \n",
    "#         for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "            #把一个batch中50个样本每个时间步的数据分别拿出来，分别计算1-10个时间步的50个样本的预测值\n",
    "            # r_out[:, time_step, :] #torch.Size([50, 32])  self.out(r_out[:, time_step, :]) 得到torch.Size([50, 1])\n",
    "            outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "#       按时间步进行stack (torch.stack(outs, dim=1)).shape: torch.Size([50, 10, 1])          \n",
    "        return torch.stack(outs, dim=1), h_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f53b253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(2, 5, batch_first=True)\n",
      "  (out): Linear(in_features=5, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbc20005",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#定义  网络 优化方法 和 损失函数\n",
    "rnn = RNN()\n",
    "print(rnn)\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=LR)   # optimize all cnn parameters\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a0a738b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 2160x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1, figsize=(30, 10))\n",
    "plt.ion()           # continuously plot\n",
    "h_state = None      # for initial hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0fe99b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Double but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m rnn \u001b[38;5;241m=\u001b[39m rnn\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m----> 2\u001b[0m prediction, h_state \u001b[38;5;241m=\u001b[39m \u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(h_state\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\d2l-zh\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, x, h_state)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, h_state):\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# x (batch, time_step, input_size)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# h_state (n_layers, batch, hidden_size) torch.Size([1, 50, 32])\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# r_out (batch, time_step, hidden_size) torch.Size([50, 10, 32])\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# x包含10个时间步的数据，但h_state是最后一个时间步的h_state，r_out包含每一个时间步的output（因为每个时间步都会有输出）\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     r_out, h_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     outs \u001b[38;5;241m=\u001b[39m []    \u001b[38;5;66;03m# save all predictions\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m#时间步(TIME_STEP)决定了h_state向后传递几步，也就是outs能使用到多少步的时序数据\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\d2l-zh\\lib\\site-packages\\torch\\nn\\modules\\module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\d2l-zh\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:471\u001b[0m, in \u001b[0;36mRNN.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRNN_TANH\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 471\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn_tanh\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    475\u001b[0m         result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mrnn_relu(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    476\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional,\n\u001b[0;32m    477\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: expected scalar type Double but found Float"
     ]
    }
   ],
   "source": [
    "rnn = rnn.float()\n",
    "prediction, h_state = rnn(train_X, h_state)\n",
    "print(h_state.reshape(-1,1))\n",
    "# plt.plot(num, h_state.data.numpy().flatten().tolist(), 'b-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67967bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练\n",
    "i = 0\n",
    "show_loss = []\n",
    "epochs = 1500\n",
    "for step in range(epochs):\n",
    "    i=i+1\n",
    "    print(\" 迭代次数: {}\".format(i))\n",
    "    #保证scalar类型为Double\n",
    "    rnn = rnn.double()\n",
    "    prediction, h_state = rnn(train_X, h_state)   # rnn output\n",
    "    # !! next step is important !!\n",
    "    .\n",
    "    h_state = h_state.data        # repack the hidden state, break the connection from last iteration\n",
    "    loss = loss_func(prediction, train_Y)         # calculate loss'\n",
    "    show_loss.append(loss.detach().numpy())\n",
    "    print(\"train loss: {}\".format(loss))\n",
    "    optimizer.zero_grad()                   # clear gradients for this training step\n",
    "    loss.backward()                         # backpropagation, compute gradients\n",
    "    optimizer.step()                        # apply gradients\n",
    "\n",
    "    #画图\n",
    "    num = list(range(50))\n",
    "    # 红色是实际，蓝色是预测\n",
    "    plt.plot(num, np.array(train_y).flatten().tolist(), 'r-')\n",
    "    plt.plot(num, prediction.data.numpy().flatten().tolist(), 'b-')\n",
    "    plt.draw(); plt.pause(0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119a12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = list(range(epochs))\n",
    "    # 红色是实际，蓝色是预测\n",
    "plt.plot(num, show_loss, 'r-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca301fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
